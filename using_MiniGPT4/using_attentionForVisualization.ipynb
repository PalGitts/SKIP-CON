{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de863d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "! pip install conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "! pip install torch==2.5.1\n",
    "! pip install bitsandbytes==0.3.0\n",
    "! pip install transformers==4.49.0\n",
    "! python3 visualization_miniGPT4.py --cfg-path '/home2/palash/p1_Jailbreak/MiniGPT4/eval_configs/minigpt4_eval.yaml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = \"/home2/palash/p1_Jailbreak/MiniGPT4/common\"\n",
    "sys.path.append(path)\n",
    "\n",
    "import os, re, gc, json, argparse, random, torch\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from minigpt4.conversation.conversation import Chat\n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.eval_utils import prepare_texts, init_model, eval_parser, computeIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91afdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = f'/home2/palash/p1_Jailbreak/LLaVA-NeXT/images/Memes/BENIGN_visual/benign_363.jpg'\n",
    "images = Image.open(image_path)\n",
    "prompt = \"[INST] <image>[/INST]\"\n",
    "inputs = processor(images, prompt, return_tensors=\"pt\").to(device)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "\n",
    "# outputs = model.generate(\n",
    "#         **inputs, \n",
    "#         max_new_tokens=1,\n",
    "#         pad_token_id=model.config.pad_token_id,\n",
    "#         eos_token_id=model.config.eos_token_id,\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_attentions=True,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<image>\"\n",
    "inputs = processor(prompt, return_tensors=\"pt\").to(device)\n",
    "print( inputs['input_ids'][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77ee418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def generate_output(prompt, images, model, processor, device_id, max_new_tokens=256):\n",
    "    \n",
    "    # prompt = \"[INST] <image>[/INST]\"\n",
    "    # image = Image.open(image_path)\n",
    "    # inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n",
    "    inputs = processor(images, prompt, return_tensors=\"pt\").to(device_id)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        return_dict_in_generate=True,\n",
    "        output_attentions=True,\n",
    "        )\n",
    "    # processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    attentions = [[att.detach().cpu() for att in attn] for attn in outputs['attentions']]\n",
    "    sequences = outputs['sequences'].detach().cpu()  # Move sequences to CPU\n",
    "    past_key_values = [[[kv.detach().cpu() for kv in layer] for layer in pkv] for pkv in outputs['past_key_values']]\n",
    "    \n",
    "    print(f'inputs: {inputs[\"input_ids\"]}')\n",
    "    with torch.no_grad():  # Ensure no gradients are computed\n",
    "        # op = model(**inputs)\n",
    "        # print(op.keys())\n",
    "        image_token_mask = (inputs[\"input_ids\"] == 32000).int().detach().cpu()\n",
    "        # image_token_mask = (model(**inputs).image_token_mask).detach().cpu()\n",
    "        \n",
    "    return (attentions, image_token_mask)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823ffd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608aa669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_cross_attention(attentions, image_token_mask):\n",
    "    \"\"\"\n",
    "    Computes the average cross-attention given to image tokens by text tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - attentions: Tensor of (num_layers, batch_size, num_heads, seq_len (query), seq_len (key))\n",
    "    - image_token_mask: Tensor of shape (batch_size, seq_len), where True indicates image tokens.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_attention_per_layer: Tensor of shape (num_layers, num_heads), average attention to image tokens.\n",
    "    \"\"\"\n",
    "    num_layers, batch_size, num_heads, seq_len, _ = attentions.shape\n",
    "    # Shape: (32, 1, 32, 586, 586)\n",
    "\n",
    "    # Remove batch dimension\n",
    "    attentions = attentions.squeeze(1)  # Shape: (32, 32, 586, 586)\n",
    "\n",
    "    # Identify image and text tokens\n",
    "    image_token_indices = torch.where(image_token_mask.squeeze(0))[0]  # Indices of image tokens\n",
    "    text_token_indices = torch.where(~image_token_mask.squeeze(0))[0]  # Indices of text tokens\n",
    "\n",
    "    # Get attention from text tokens to image tokens: (num_layers, num_heads, num_text_tokens, num_image_tokens)\n",
    "    text_to_image_attn = attentions[:, :, text_token_indices, :][:, :, :, image_token_indices]\n",
    "\n",
    "    # Average across text tokens, image tokens and heads: Shape (num_layers)\n",
    "    avg_attention_per_layer = text_to_image_attn.mean(dim=(1, 2, 3))\n",
    "    \n",
    "    return avg_attention_per_layer  # Shape: (num_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All special tokens:\", processor.tokenizer.all_special_tokens)\n",
    "print(processor.tokenizer('<image>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1400684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_benignImages = f'/home2/palash/p1_Jailbreak/LLaVA-NeXT/images/Memes/BENIGN_visual/'\n",
    "path_toxicImages = f'/home2/palash/p1_Jailbreak/LLaVA-NeXT/images/Memes/TOXIC_visuals/'\n",
    "path_memeImages = f'/home2/palash/p1_Jailbreak/LLaVA-NeXT/images/Memes/ISLAM_memes/'\n",
    "\n",
    "\n",
    "all_benignImages = os.listdir(path_benignImages)\n",
    "print(all_benignImages)\n",
    "\n",
    "all_toxicImages = os.listdir(path_toxicImages)\n",
    "print(all_toxicImages)\n",
    "\n",
    "all_memeImages = os.listdir(path_memeImages)\n",
    "print(all_memeImages)\n",
    "\n",
    "\n",
    "query_list = []\n",
    "with open(f'../jb_dataset/BeaverTails/BeaverTailsTestCases_100.txt', 'r') as f:\n",
    "        queries = f.readlines()\n",
    "        query_list = [i.replace('\\n', '') for i in queries]\n",
    "\n",
    "print(f'*** A total of {len(query_list)} queries.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e85416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "benign_list = []\n",
    "toxic_list = []\n",
    "memes_list = []\n",
    "\n",
    "for idx, ip in enumerate( query_list):\n",
    "    \n",
    "    prompt = f\"<image>\\nUSER: Answer the following question:\\n{ip} ASSISTANT:\" \n",
    "    \n",
    "    path = path_benignImages + random.choice(all_benignImages)\n",
    "    benign_images = [Image.open(path).convert('RGB')]\n",
    "\n",
    "    path = path_toxicImages + random.choice(all_toxicImages)\n",
    "    toxic_images = [Image.open(path).convert('RGB')]\n",
    "\n",
    "    path = path_memeImages + random.choice(all_memeImages)\n",
    "    meme_images = [Image.open(path).convert('RGB')]\n",
    "\n",
    "    benign_list.append((prompt, benign_images))\n",
    "    toxic_list.append((prompt, toxic_images))\n",
    "    memes_list.append((prompt, meme_images))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec881685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(benign_list))\n",
    "print( len(toxic_list))\n",
    "print( len(memes_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea547b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_attns = []\n",
    "\n",
    "# Process benign inputs\n",
    "i=0\n",
    "for prompt, image in benign_list:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    attn , i_t_m = generate_output(prompt, image, model, processor, device, max_new_tokens=1)\n",
    "    attn = torch.stack(attn[0])\n",
    "    attn = compute_average_cross_attention(attn, i_t_m)\n",
    "    benign_attns.append(attn)\n",
    "    print(f\"Output of benign prompt {i} generated\")\n",
    "\n",
    "torch.save(benign_attns,\"benign_attention_weights.pt\")\n",
    "benign_attns = [] # Clear the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866b0ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_attns = []\n",
    "\n",
    "# Process toxic inputs\n",
    "i=0\n",
    "for prompt, image in toxic_list:\n",
    "    i+=1\n",
    "    attn , i_t_m = generate_output(prompt, image, model, processor, device, max_new_tokens=1)\n",
    "    attn = torch.stack(attn[0])\n",
    "    attn = compute_average_cross_attention(attn, i_t_m)\n",
    "    toxic_attns.append(attn)\n",
    "    print(f\"Output of toxic prompt {i} generated\")\n",
    "\n",
    "torch.save(toxic_attns,\"toxic_attention_weights.pt\")\n",
    "toxic_attns = [] # Clear the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memes_attns = []\n",
    "\n",
    "# Process memes inputs\n",
    "i=0\n",
    "for prompt, image in memes_list:\n",
    "    i+=1\n",
    "    print(i)\n",
    "    attn , i_t_m = generate_output(prompt, image, model, processor, device, max_new_tokens=1)\n",
    "    attn = torch.stack(attn[0])\n",
    "    attn = compute_average_cross_attention(attn, i_t_m)\n",
    "    memes_attns.append(attn)\n",
    "    print(f\"Output of memes prompt {i} generated\")\n",
    "\n",
    "torch.save(memes_attns,\"memes_attention_weights.pt\")\n",
    "memes_attns = [] # Clear the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b758db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_scores(toxic_attentions, benign_attentions, name):\n",
    "    \"\"\"\n",
    "    Plots attention scores for toxic and benign inputs.\n",
    "\n",
    "    Args:\n",
    "        toxic_attentions (list of torch.Tensor): List of attention tensors for toxic inputs.\n",
    "        benign_attentions (list of torch.Tensor): List of attention tensors for benign inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Convert lists of tensors to a single tensor for easier manipulation\n",
    "    toxic_attentions_tensor = torch.stack(toxic_attentions)  # Shape: [num_samples, 32]\n",
    "    benign_attentions_tensor = torch.stack(benign_attentions)  # Shape: [num_samples, 32]\n",
    "\n",
    "    # Number of layers\n",
    "    num_layers = toxic_attentions_tensor.shape[1]\n",
    "\n",
    "    toxic_attentions_tensor = toxic_attentions_tensor.mean(dim=0) # Shape: [32]\n",
    "    benign_attentions_tensor = benign_attentions_tensor.mean(dim=0) # Shape: [32]\n",
    "\n",
    "    # Create a scatter plot for each layer\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # for layer in range(num_layers):\n",
    "    #     # Plot toxic attention scores\n",
    "    #     plt.scatter(\n",
    "    #         [layer] * len(toxic_attentions_tensor),  # X-axis: layer index\n",
    "    #         toxic_attentions_tensor[:, layer],  # Y-axis: attention scores\n",
    "    #         color='red',  # Color for toxic points\n",
    "    #         label='Toxic' if layer == 0 else None,  # Label only once\n",
    "    #         alpha=0.6  # Transparency\n",
    "    #     )\n",
    "    #     # Plot benign attention scores\n",
    "    #     plt.scatter(\n",
    "    #         [layer] * len(benign_attentions_tensor),  # X-axis: layer index\n",
    "    #         benign_attentions_tensor[:, layer],  # Y-axis: attention scores\n",
    "    #         color='blue',  # Color for benign points\n",
    "    #         label='Benign' if layer == 0 else None,  # Label only once\n",
    "    #         alpha=0.6  # Transparency\n",
    "    #     )\n",
    "    layers = list(range(1, num_layers+1))\n",
    "    plt.plot(layers, benign_attentions_tensor, marker='s', linestyle='none', label='Benign Image', color='blue')\n",
    "    plt.plot(layers, toxic_attentions_tensor, marker='s', linestyle='none', label='Meme Image', color='red')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Attention Score')\n",
    "    plt.title('Average Cross Attention Scores Across Layers')\n",
    "    plt.xticks(range(1, num_layers+1))  # Set x-ticks to layer indices\n",
    "    plt.legend()  # Show legend\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)  # Add grid lines\n",
    "    plt.savefig(f'attention_scores_for{name}.png')  # Save the plot as a PNG file\n",
    "    print(\"Plot saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3afc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Visualizing Average Cross Attentions\")\n",
    "\n",
    "toxic_attns = torch.load(\"./toxic_attention_weights.pt\")\n",
    "benign_attns = torch.load(\"./benign_attention_weights.pt\")\n",
    "    \n",
    "plot_attention_scores(toxic_attns, benign_attns, 'benVsToxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5805062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "print(\"Visualizing Average Cross Attentions\")\n",
    "\n",
    "memes_attns = torch.load(\"./memes_attention_weights.pt\")\n",
    "benign_attns = torch.load(\"./benign_attention_weights.pt\")\n",
    "    \n",
    "plot_attention_scores(memes_attns, benign_attns, 'benVsMemes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5179b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toxic_list = []\n",
    "benign_list = []\n",
    "\n",
    "image_paths = [\"jb_memes/benign_5.jpg\", \"jb_memes/benign_2.jpg\", \"jb_memes/benign_3.jpg\", \"jb_memes/benign_4.jpg\", \"jb_memes/benign_1.jpg\"]\n",
    "benign_images = [Image.open(image_path).convert('RGB') for image_path in image_paths]\n",
    "\n",
    "with open(\"jb_outputs_refined.csv\", 'r', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader) # Skip the header\n",
    "\n",
    "    for row in reader:\n",
    "        text_input, benign_output, jb_output, meme_images_str = row\n",
    "        meme_images = [meme_image.strip() for meme_image in meme_images_str.split(\";\")]\n",
    "        meme_images = [Image.open(meme_image).convert('RGB') for meme_image in meme_images]\n",
    "        prompt = f\"USER: <image>\\n<image>\\n<image>\\n<image>\\n<image>\\nAnswer the following question:\\n{text_input} ASSISTANT:\"\n",
    "        toxic_list.append((prompt, meme_images))\n",
    "        benign_list.append((prompt, benign_images))\n",
    "\n",
    "# Debugging\n",
    "benign_list = benign_list[20:40]\n",
    "toxic_list = toxic_list[20:40]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_attns = []\n",
    "benign_attns = []\n",
    "\n",
    "# Process toxic inputs\n",
    "i=0\n",
    "for prompt, image in toxic_list:\n",
    "    i+=1\n",
    "    attn , i_t_m = generate_output(prompt, image, model, processor, device_id)\n",
    "    attn = torch.stack(attn[0])\n",
    "    attn = compute_average_cross_attention(attn, i_t_m)\n",
    "    toxic_attns.append(attn)\n",
    "    print(f\"Output of toxic prompt {i} generated\")\n",
    "\n",
    "torch.save(toxic_attns,\"toxic_attention_weights_20-39.pt\")\n",
    "toxic_attns = [] # Clear the weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e5bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jb_miniGPT4",
   "language": "python",
   "name": "jb_minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
